{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SV-SCN Training on Google Colab\n",
    "\n",
    "This notebook trains the Single-View Shape Completion Network on Google Colab's free GPU.\n",
    "\n",
    "**Steps:**\n",
    "1. Setup environment & install dependencies\n",
    "2. Clone/upload project code\n",
    "3. Generate placeholder training data\n",
    "4. Train the model\n",
    "5. Download trained checkpoint\n",
    "\n",
    "**Runtime:** ~2-3 hours for full training (150 epochs, 500 samples)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Check GPU & Install Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check GPU availability\n",
    "!nvidia-smi\n",
    "\n",
    "import torch\n",
    "print(f\"\\nPyTorch version: {torch.__version__}\")\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"CUDA version: {torch.version.cuda}\")\n",
    "    print(f\"Device name: {torch.cuda.get_device_name(0)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Clone Project (or Upload Files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Option A: Clone from GitHub (replace with your repo URL)\n",
    "# !git clone https://github.com/yourusername/frozo-3d-model.git\n",
    "# %cd frozo-3d-model\n",
    "\n",
    "# Option B: Upload project folder manually\n",
    "# Use Colab's file browser to upload the entire frozo-3d-model folder\n",
    "\n",
    "# For now, let's create the project structure\n",
    "import os\n",
    "os.makedirs('frozo-3d-model', exist_ok=True)\n",
    "%cd frozo-3d-model\n",
    "\n",
    "# You'll need to upload these folders:\n",
    "# - svscn/\n",
    "# - scripts/\n",
    "# - requirements.txt\n",
    "\n",
    "print(\"\\n==> Upload your project files using the file browser on the left\")\n",
    "print(\"    Or clone from GitHub if you have it in a repo\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Install Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required packages\n",
    "!pip install open3d>=0.17.0\n",
    "!pip install trimesh>=4.0.0\n",
    "!pip install scipy>=1.10.0\n",
    "!pip install objaverse>=0.1.7\n",
    "!pip install tensorboard>=2.14.0\n",
    "!pip install pytest>=7.4.0\n",
    "\n",
    "print(\"\\n✅ All dependencies installed!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: Generate Placeholder Data\n",
    "\n",
    "This creates synthetic furniture meshes for training/testing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate placeholder ShapeNet-like data\n",
    "!python -m svscn.data.shapenet \\\n",
    "    --placeholder \\\n",
    "    --output_dir data/shapenet \\\n",
    "    --samples_per_category 167\n",
    "\n",
    "print(\"\\n✅ Generated 500 placeholder furniture meshes\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocess: Mesh → Point Cloud\n",
    "!python -m svscn.data.preprocess \\\n",
    "    --input_dir data/shapenet \\\n",
    "    --output_dir data/processed \\\n",
    "    --num_points 8192\n",
    "\n",
    "print(\"\\n✅ Converted meshes to point clouds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate training pairs (partial + full)\n",
    "!python -m svscn.data.augment \\\n",
    "    --input_dir data/processed \\\n",
    "    --output_dir data/training \\\n",
    "    --views 3\n",
    "\n",
    "print(\"\\n✅ Generated training pairs (partial + complete point clouds)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5: Quick Test (Optional)\n",
    "\n",
    "Test the model architecture before training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Quick model test\n",
    "import torch\n",
    "from svscn.models import SVSCN\n",
    "\n",
    "model = SVSCN(num_classes=3)\n",
    "print(f\"Model created successfully\")\n",
    "\n",
    "# Test forward pass\n",
    "batch_size = 4\n",
    "partial = torch.randn(batch_size, 2048, 3)\n",
    "class_ids = torch.randint(0, 3, (batch_size,))\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    model = model.cuda()\n",
    "    partial = partial.cuda()\n",
    "    class_ids = class_ids.cuda()\n",
    "\n",
    "completed = model(partial, class_ids)\n",
    "\n",
    "print(f\"\\nInput shape: {partial.shape}\")\n",
    "print(f\"Output shape: {completed.shape}\")\n",
    "print(f\"✅ Model test passed!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 6: Train Model\n",
    "\n",
    "### Quick Training (10 epochs, ~5-10 min)\n",
    "For testing the pipeline:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Quick training for testing\n",
    "!python scripts/train.py \\\n",
    "    --data_dir data/training \\\n",
    "    --epochs 10 \\\n",
    "    --batch_size 32 \\\n",
    "    --checkpoint_dir checkpoints \\\n",
    "    --log_dir logs \\\n",
    "    --device cuda"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Full Training (150 epochs, ~2-3 hours)\n",
    "For production model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Full training (uncomment to run)\n",
    "# WARNING: This will take 2-3 hours\n",
    "\n",
    "!python scripts/train.py \\\n",
    "    --data_dir data/training \\\n",
    "    --epochs 150 \\\n",
    "    --batch_size 32 \\\n",
    "    --checkpoint_dir checkpoints \\\n",
    "    --log_dir logs \\\n",
    "    --device cuda"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 7: Monitor Training (TensorBoard)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load TensorBoard\n",
    "%load_ext tensorboard\n",
    "%tensorboard --logdir logs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 8: Test Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a test partial cloud\n",
    "import numpy as np\n",
    "\n",
    "# Generate random partial cloud\n",
    "partial = np.random.randn(2048, 3).astype(np.float32)\n",
    "partial = (partial - partial.mean(axis=0)) / partial.std()\n",
    "\n",
    "np.save('test_partial.npy', partial)\n",
    "print(f\"Created test partial cloud: {partial.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run inference\n",
    "!python scripts/infer.py \\\n",
    "    --checkpoint checkpoints/best.pt \\\n",
    "    --input test_partial.npy \\\n",
    "    --output completed.npy \\\n",
    "    --class_id 0 \\\n",
    "    --device cuda\n",
    "\n",
    "print(\"\\n✅ Inference complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 9: Visualize Results (Optional)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize using matplotlib\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "\n",
    "partial = np.load('test_partial.npy')\n",
    "completed = np.load('completed.npy')\n",
    "\n",
    "fig = plt.figure(figsize=(15, 5))\n",
    "\n",
    "# Partial cloud\n",
    "ax1 = fig.add_subplot(131, projection='3d')\n",
    "ax1.scatter(partial[:, 0], partial[:, 1], partial[:, 2], c='blue', s=1)\n",
    "ax1.set_title('Input (Partial)')\n",
    "ax1.set_xlabel('X')\n",
    "ax1.set_ylabel('Y')\n",
    "ax1.set_zlabel('Z')\n",
    "\n",
    "# Completed cloud\n",
    "ax2 = fig.add_subplot(132, projection='3d')\n",
    "ax2.scatter(completed[:, 0], completed[:, 1], completed[:, 2], c='green', s=1)\n",
    "ax2.set_title('Output (Completed)')\n",
    "ax2.set_xlabel('X')\n",
    "ax2.set_ylabel('Y')\n",
    "ax2.set_zlabel('Z')\n",
    "\n",
    "# Both overlaid\n",
    "ax3 = fig.add_subplot(133, projection='3d')\n",
    "ax3.scatter(partial[:, 0], partial[:, 1], partial[:, 2], c='blue', s=1, alpha=0.5, label='Partial')\n",
    "ax3.scatter(completed[:, 0], completed[:, 1], completed[:, 2], c='green', s=1, alpha=0.3, label='Completed')\n",
    "ax3.set_title('Overlay')\n",
    "ax3.set_xlabel('X')\n",
    "ax3.set_ylabel('Y')\n",
    "ax3.set_zlabel('Z')\n",
    "ax3.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"Partial points: {len(partial)}\")\n",
    "print(f\"Completed points: {len(completed)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 10: Download Trained Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download the best checkpoint to your local machine\n",
    "from google.colab import files\n",
    "\n",
    "# Download checkpoint\n",
    "files.download('checkpoints/best.pt')\n",
    "\n",
    "print(\"\\n✅ Model downloaded! You can now use it locally for inference.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 11: Export as Mesh (Optional)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Export as GLB for AR\n",
    "!python scripts/infer.py \\\n",
    "    --checkpoint checkpoints/best.pt \\\n",
    "    --input test_partial.npy \\\n",
    "    --output completed.glb \\\n",
    "    --export_mesh \\\n",
    "    --class_id 0 \\\n",
    "    --device cuda\n",
    "\n",
    "# Download GLB file\n",
    "files.download('completed.glb')\n",
    "\n",
    "print(\"\\n✅ GLB file downloaded! You can view it in 3D viewers or AR apps.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "You've successfully:\n",
    "1. ✅ Set up the environment on Colab GPU\n",
    "2. ✅ Generated placeholder training data (500 samples)\n",
    "3. ✅ Trained the SV-SCN model\n",
    "4. ✅ Tested inference\n",
    "5. ✅ Downloaded the trained checkpoint\n",
    "\n",
    "### Next Steps:\n",
    "- Use the downloaded `best.pt` checkpoint locally for inference\n",
    "- For production: Train on real ShapeNet/Objaverse data (5k-10k samples)\n",
    "- Export models as GLB for web/mobile AR applications\n",
    "\n",
    "### Local Inference (after downloading checkpoint):\n",
    "```bash\n",
    "python scripts/infer.py \\\n",
    "    --checkpoint best.pt \\\n",
    "    --input your_partial.npy \\\n",
    "    --output result.glb \\\n",
    "    --export_mesh \\\n",
    "    --device cpu\n",
    "```"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.9.0"
  },
  "accelerator": "GPU"
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
