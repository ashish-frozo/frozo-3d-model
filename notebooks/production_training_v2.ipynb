{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# SV-SCN Production Training v2\n",
                "\n",
                "**Improved with error handling and validation**\n",
                "\n",
                "- 500 training samples\n",
                "- 150 epochs (~2-3 hours)\n",
                "- Automatic checkpoint detection\n",
                "- Complete error checking\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Step 1: Check GPU"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "!nvidia-smi\n",
                "\n",
                "import torch\n",
                "print(f\"\\nPyTorch: {torch.__version__}\")\n",
                "print(f\"CUDA: {torch.cuda.is_available()}\")\n",
                "if torch.cuda.is_available():\n",
                "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
                "else:\n",
                "    print(\"⚠️ Enable GPU: Runtime → Change runtime type → GPU\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Step 2: Clone Project"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "!git clone https://github.com/ashish-frozo/frozo-3d-model.git\n",
                "%cd frozo-3d-model\n",
                "\n",
                "!ls -la svscn/ scripts/ 2>&1 || echo \"❌ Project structure not found!\""
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Step 3: Install Dependencies"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "!pip install -q open3d>=0.17.0 trimesh>=4.0.0 scipy>=1.10.0\n",
                "!pip install -q objaverse>=0.1.7 tensorboard>=2.14.0\n",
                "\n",
                "from svscn.models import SVSCN\n",
                "from svscn.config import default_config\n",
                "\n",
                "print(f\"✅ Setup complete - Version: {default_config.VERSION}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Step 4: Clean Previous Data (if any)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "!rm -rf data/shapenet_500 data/processed_500 data/training_500\n",
                "!mkdir -p data\n",
                "\n",
                "print(\"✅ Clean slate ready\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Step 5: Generate Training Data (WITH ERROR CHECKING)\n",
                "\n",
                "This will create ~500 samples with validation at each step"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Step 5a: Generate meshes\n",
                "!python -m svscn.data.shapenet \\\n",
                "    --placeholder \\\n",
                "    --output_dir data/shapenet_500 \\\n",
                "    --samples_per_category 167\n",
                "\n",
                "# VERIFY\n",
                "import subprocess\n",
                "import sys\n",
                "\n",
                "result = subprocess.run(['find', 'data/shapenet_500', '-name', '*.obj'], \n",
                "                       capture_output=True, text=True)\n",
                "obj_files = [f for f in result.stdout.strip().split('\\n') if f]\n",
                "num_obj = len(obj_files)\n",
                "\n",
                "print(f\"\\n{'='*50}\")\n",
                "print(f\"STEP 5a: Generate Meshes\")\n",
                "print(f\"{'='*50}\")\n",
                "print(f\"OBJ files: {num_obj}\")\n",
                "print(f\"Expected: ~500\")\n",
                "\n",
                "if num_obj < 100:\n",
                "    print(f\"❌ ERROR: Only {num_obj} meshes!\")\n",
                "    raise ValueError(f\"Expected 500+, got {num_obj}\")\n",
                "else:\n",
                "    print(f\"✅ SUCCESS\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Step 5b: Preprocess\n",
                "!python -m svscn.data.preprocess \\\n",
                "    --input_dir data/shapenet_500 \\\n",
                "    --output_dir data/processed_500 \\\n",
                "    --num_points 8192\n",
                "\n",
                "# VERIFY\n",
                "result = subprocess.run(['find', 'data/processed_500', '-name', '*.npy'], \n",
                "                       capture_output=True, text=True)\n",
                "pc_files = [f for f in result.stdout.strip().split('\\n') if f]\n",
                "num_pc = len(pc_files)\n",
                "\n",
                "print(f\"\\n{'='*50}\")\n",
                "print(f\"STEP 5b: Preprocess\")\n",
                "print(f\"{'='*50}\")\n",
                "print(f\"Point clouds: {num_pc}\")\n",
                "print(f\"Expected: ~{num_obj}\")\n",
                "\n",
                "if num_pc < num_obj * 0.9:  # At least 90% success rate\n",
                "    print(f\"❌ ERROR: Only {num_pc}/{num_obj} processed!\")\n",
                "    raise ValueError(\"Too many preprocessing failures\")\n",
                "else:\n",
                "    print(f\"✅ SUCCESS\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Step 5c: Augment\n",
                "!python -m svscn.data.augment \\\n",
                "    --input_dir data/processed_500 \\\n",
                "    --output_dir data/training_500 \\\n",
                "    --views 3\n",
                "\n",
                "# VERIFY\n",
                "result_full = subprocess.run(['find', 'data/training_500/full', '-name', '*.npy'], \n",
                "                             capture_output=True, text=True)\n",
                "result_partial = subprocess.run(['find', 'data/training_500/partial', '-name', '*.npy'], \n",
                "                                capture_output=True, text=True)\n",
                "\n",
                "full_files = [f for f in result_full.stdout.strip().split('\\n') if f]\n",
                "partial_files = [f for f in result_partial.stdout.strip().split('\\n') if f]\n",
                "\n",
                "num_full = len(full_files)\n",
                "num_partial = len(partial_files)\n",
                "\n",
                "print(f\"\\n{'='*50}\")\n",
                "print(f\"STEP 5c: Create Training Pairs\")\n",
                "print(f\"{'='*50}\")\n",
                "print(f\"Full clouds: {num_full}\")\n",
                "print(f\"Partial clouds: {num_partial}\")\n",
                "print(f\"Expected: ~{num_pc * 3} each\")\n",
                "print(f\"Unique samples: {num_full // 3}\")\n",
                "\n",
                "if num_full < 300 or num_partial < 300:\n",
                "    print(f\"❌ ERROR: Not enough training pairs!\")\n",
                "    raise ValueError(f\"Expected 300+, got {num_full}/{num_partial}\")\n",
                "elif abs(num_full - num_partial) > 5:\n",
                "    print(f\"⚠️ WARNING: Count mismatch!\")\n",
                "else:\n",
                "    print(f\"✅ SUCCESS - Ready for training!\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Step 6: Create Splits"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "import numpy as np\n",
                "from pathlib import Path\n",
                "\n",
                "training_dir = Path('data/training_500')\n",
                "full_dir = training_dir / 'full'\n",
                "\n",
                "samples = set()\n",
                "for f in full_dir.glob('*_full.npy'):\n",
                "    name = f.stem.replace('_full', '')\n",
                "    base = '_'.join(name.split('_')[:-1])\n",
                "    samples.add(base)\n",
                "\n",
                "samples = sorted(list(samples))\n",
                "np.random.seed(42)\n",
                "np.random.shuffle(samples)\n",
                "\n",
                "n = len(samples)\n",
                "train = samples[:int(0.8*n)]\n",
                "val = samples[int(0.8*n):int(0.9*n)]\n",
                "test = samples[int(0.9*n):]\n",
                "\n",
                "splits_dir = training_dir / 'splits'\n",
                "splits_dir.mkdir(exist_ok=True)\n",
                "\n",
                "(splits_dir / 'train.txt').write_text('\\n'.join(train))\n",
                "(splits_dir / 'val.txt').write_text('\\n'.join(val))\n",
                "(splits_dir / 'test.txt').write_text('\\n'.join(test))\n",
                "\n",
                "print(f\"✅ Splits: {len(train)} train, {len(val)} val, {len(test)} test\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Step 7: Train (150 epochs, ~2-3 hours)\n",
                "\n",
                "For quick test: change `--epochs 150` to `--epochs 10`"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Create directories\n",
                "!mkdir -p checkpoints_prod logs_prod\n",
                "\n",
                "# Train\n",
                "!python scripts/train.py \\\n",
                "    --data_dir data/training_500 \\\n",
                "    --epochs 150 \\\n",
                "    --batch_size 32 \\\n",
                "    --checkpoint_dir checkpoints_prod \\\n",
                "    --log_dir logs_prod \\\n",
                "    --device cuda\n",
                "\n",
                "print(\"\\n✅ Training complete!\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Step 8: Monitor Training"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "%load_ext tensorboard\n",
                "%tensorboard --logdir logs_prod"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Step 9: Auto-Find Checkpoint"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "import glob\n",
                "\n",
                "checkpoint_files = glob.glob('checkpoints_prod/*/best.pt')\n",
                "\n",
                "if not checkpoint_files:\n",
                "    print(\"❌ No checkpoint found!\")\n",
                "    CP = None\n",
                "else:\n",
                "    CP = sorted(checkpoint_files)[-1]\n",
                "    print(f\"✅ Checkpoint: {CP}\")\n",
                "    !ls -lh {CP}"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Step 10-13: Test, Visualize, Export, Download"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Test input\n",
                "import numpy as np\n",
                "partial = np.random.randn(2048, 3).astype(np.float32)\n",
                "partial = (partial - partial.mean(axis=0)) / partial.std()\n",
                "np.save('test.npy', partial)\n",
                "\n",
                "# Inference\n",
                "if CP:\n",
                "    !python scripts/infer.py --checkpoint {CP} --input test.npy --output out.npy --class_id 0 --device cuda\n",
                "    !python scripts/infer.py --checkpoint {CP} --input test.npy --output out.glb --export_mesh --class_id 0 --device cuda\n",
                "    \n",
                "    # Download\n",
                "    from google.colab import files\n",
                "    files.download(CP)\n",
                "    files.download('out.glb')\n",
                "    \n",
                "    print(\"\\n✅ All done!\")"
            ]
        }
    ],
    "metadata": {
        "accelerator": "GPU",
        "colab": {
            "provenance": [],
            "gpuType": "T4"
        },
        "kernelspec": {
            "display_name": "Python 3",
            "name": "python3"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 0
}