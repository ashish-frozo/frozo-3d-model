{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# SV-SCN Training with Real Furniture Data (Objaverse)\n",
                "\n",
                "**Production-Ready Training with Realistic Furniture Models**\n",
                "\n",
                "- ‚úÖ Real furniture data (chairs with legs!)\n",
                "- ‚úÖ Automatic error handling\n",
                "- ‚úÖ Comprehensive validation\n",
                "- ‚úÖ 150 epochs for production quality\n",
                "- ‚úÖ Automatic checkpoint detection\n",
                "\n",
                "**Expected Output:** Realistic 3D furniture models with proper details"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Step 1: Check GPU & Environment"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "import sys\n",
                "\n",
                "# Check GPU\n",
                "!nvidia-smi\n",
                "\n",
                "import torch\n",
                "print(f\"\\nPyTorch: {torch.__version__}\")\n",
                "print(f\"CUDA Available: {torch.cuda.is_available()}\")\n",
                "\n",
                "if torch.cuda.is_available():\n",
                "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
                "    print(f\"GPU Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.2f} GB\")\n",
                "    print(\"‚úÖ GPU Ready!\")\n",
                "else:\n",
                "    print(\"‚ùå NO GPU! Enable: Runtime ‚Üí Change runtime type ‚Üí GPU\")\n",
                "    sys.exit(1)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Step 2: Clone Project from GitHub"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "import os\n",
                "\n",
                "# Clone project\n",
                "if os.path.exists('frozo-3d-model'):\n",
                "    print(\"‚ö†Ô∏è Project already exists, pulling latest...\")\n",
                "    !cd frozo-3d-model && git pull\n",
                "else:\n",
                "    !git clone https://github.com/ashish-frozo/frozo-3d-model.git\n",
                "\n",
                "# Navigate to project\n",
                "%cd frozo-3d-model\n",
                "\n",
                "# Verify structure\n",
                "if not os.path.exists('svscn') or not os.path.exists('scripts'):\n",
                "    print(\"‚ùå Project structure incorrect!\")\n",
                "    sys.exit(1)\n",
                "\n",
                "print(\"‚úÖ Project cloned successfully!\")\n",
                "!pwd"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Step 3: Install Dependencies"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Install core dependencies\n",
                "!pip install -q open3d>=0.17.0 trimesh>=4.0.0 scipy>=1.10.0\n",
                "!pip install -q tensorboard>=2.14.0\n",
                "\n",
                "# Install Objaverse for real furniture data\n",
                "!pip install -q objaverse\n",
                "\n",
                "# Verify imports\n",
                "try:\n",
                "    from svscn.models import SVSCN\n",
                "    from svscn.config import default_config\n",
                "    import objaverse\n",
                "    print(f\"‚úÖ All dependencies installed!\")\n",
                "    print(f\"Project version: {default_config.VERSION}\")\n",
                "except ImportError as e:\n",
                "    print(f\"‚ùå Import failed: {e}\")\n",
                "    sys.exit(1)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Step 4: Download Real Furniture from Objaverse\n",
                "\n",
                "This downloads **real furniture models** instead of placeholder boxes!"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "import objaverse\n",
                "import subprocess\n",
                "from pathlib import Path\n",
                "import json\n",
                "\n",
                "print(\"üì• Downloading real furniture from Objaverse...\")\n",
                "print(\"This may take 30-60 minutes for 500 models\\n\")\n",
                "\n",
                "# Create output directory\n",
                "!mkdir -p data/objaverse_raw\n",
                "\n",
                "# Download furniture annotations\n",
                "annotations = objaverse.load_annotations()\n",
                "\n",
                "# Filter for furniture categories\n",
                "furniture_objects = []\n",
                "for uid, data in annotations.items():\n",
                "    tags = data.get('tags', [])\n",
                "    name = data.get('name', '').lower()\n",
                "    \n",
                "    # Filter for chairs, tables, stools\n",
                "    if any(keyword in name for keyword in ['chair', 'stool', 'table', 'desk', 'seat']):\n",
                "        furniture_objects.append(uid)\n",
                "\n",
                "print(f\"Found {len(furniture_objects)} furniture objects in Objaverse\")\n",
                "\n",
                "# Download 500 samples (167 per category approximate)\n",
                "target_count = 500\n",
                "selected_uids = furniture_objects[:target_count]\n",
                "\n",
                "print(f\"Downloading {len(selected_uids)} models...\")\n",
                "\n",
                "# Download\n",
                "objects = objaverse.load_objects(\n",
                "    uids=selected_uids,\n",
                "    download_processes=4\n",
                ")\n",
                "\n",
                "# Move to our data directory and organize\n",
                "output_dir = Path('data/objaverse_raw')\n",
                "chair_dir = output_dir / 'chair'\n",
                "table_dir = output_dir / 'table'\n",
                "stool_dir = output_dir / 'stool'\n",
                "\n",
                "chair_dir.mkdir(exist_ok=True)\n",
                "table_dir.mkdir(exist_ok=True)\n",
                "stool_dir.mkdir(exist_ok=True)\n",
                "\n",
                "# Organize by category\n",
                "for uid, filepath in objects.items():\n",
                "    annotation = annotations[uid]\n",
                "    name = annotation.get('name', '').lower()\n",
                "    \n",
                "    # Copy to appropriate category\n",
                "    if 'chair' in name or 'seat' in name:\n",
                "        !cp {filepath} {chair_dir}/{uid}.glb\n",
                "    elif 'table' in name or 'desk' in name:\n",
                "        !cp {filepath} {table_dir}/{uid}.glb\n",
                "    else:\n",
                "        !cp {filepath} {stool_dir}/{uid}.glb\n",
                "\n",
                "# Verify download\n",
                "chair_count = len(list(chair_dir.glob('*.glb')))\n",
                "table_count = len(list(table_dir.glob('*.glb')))\n",
                "stool_count = len(list(stool_dir.glob('*.glb')))\n",
                "\n",
                "print(f\"\\n{'='*50}\")\n",
                "print(f\"DOWNLOAD COMPLETE\")\n",
                "print(f\"{'='*50}\")\n",
                "print(f\"Chairs: {chair_count}\")\n",
                "print(f\"Tables: {table_count}\")\n",
                "print(f\"Stools: {stool_count}\")\n",
                "print(f\"Total: {chair_count + table_count + stool_count}\")\n",
                "\n",
                "if (chair_count + table_count + stool_count) < 100:\n",
                "    print(\"‚ùå ERROR: Not enough models downloaded!\")\n",
                "    sys.exit(1)\n",
                "else:\n",
                "    print(\"‚úÖ Real furniture data ready!\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Step 5: Convert GLB to OBJ Format"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "import trimesh\n",
                "from pathlib import Path\n",
                "from tqdm import tqdm\n",
                "\n",
                "print(\"üîÑ Converting GLB to OBJ format...\")\n",
                "\n",
                "output_dir = Path('data/shapenet_objaverse')\n",
                "output_dir.mkdir(exist_ok=True)\n",
                "\n",
                "for category in ['chair', 'table', 'stool']:\n",
                "    input_dir = Path(f'data/objaverse_raw/{category}')\n",
                "    category_dir = output_dir / category\n",
                "    category_dir.mkdir(exist_ok=True)\n",
                "    \n",
                "    glb_files = list(input_dir.glob('*.glb'))\n",
                "    print(f\"\\nProcessing {len(glb_files)} {category} models...\")\n",
                "    \n",
                "    converted = 0\n",
                "    for glb_file in tqdm(glb_files):\n",
                "        try:\n",
                "            # Load and convert\n",
                "            mesh = trimesh.load(glb_file)\n",
                "            obj_path = category_dir / f\"{glb_file.stem}.obj\"\n",
                "            mesh.export(obj_path)\n",
                "            converted += 1\n",
                "        except Exception as e:\n",
                "            print(f\"‚ö†Ô∏è Failed to convert {glb_file.name}: {e}\")\n",
                "            continue\n",
                "    \n",
                "    print(f\"‚úÖ {category}: {converted}/{len(glb_files)} converted\")\n",
                "\n",
                "print(\"\\n‚úÖ Conversion complete!\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Step 6: Preprocess to Point Clouds"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Preprocess OBJ files to point clouds\n",
                "!python -m svscn.data.preprocess \\\n",
                "    --input_dir data/shapenet_objaverse \\\n",
                "    --output_dir data/processed_objaverse \\\n",
                "    --num_points 8192\n",
                "\n",
                "# Verify\n",
                "result = subprocess.run(['find', 'data/processed_objaverse', '-name', '*.npy'], \n",
                "                       capture_output=True, text=True)\n",
                "pc_files = [f for f in result.stdout.strip().split('\\n') if f]\n",
                "num_pc = len(pc_files)\n",
                "\n",
                "print(f\"\\n{'='*50}\")\n",
                "print(f\"PREPROCESSING COMPLETE\")\n",
                "print(f\"{'='*50}\")\n",
                "print(f\"Point clouds created: {num_pc}\")\n",
                "\n",
                "if num_pc < 100:\n",
                "    print(\"‚ùå ERROR: Not enough point clouds!\")\n",
                "    sys.exit(1)\n",
                "else:\n",
                "    print(\"‚úÖ SUCCESS\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Step 7: Generate Training Pairs"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Create training pairs (partial + full)\n",
                "!python -m svscn.data.augment \\\n",
                "    --input_dir data/processed_objaverse \\\n",
                "    --output_dir data/training_objaverse \\\n",
                "    --views 3\n",
                "\n",
                "# Verify\n",
                "result_full = subprocess.run(['find', 'data/training_objaverse/full', '-name', '*.npy'], \n",
                "                             capture_output=True, text=True)\n",
                "result_partial = subprocess.run(['find', 'data/training_objaverse/partial', '-name', '*.npy'], \n",
                "                                capture_output=True, text=True)\n",
                "\n",
                "full_files = [f for f in result_full.stdout.strip().split('\\n') if f]\n",
                "partial_files = [f for f in result_partial.stdout.strip().split('\\n') if f]\n",
                "\n",
                "num_full = len(full_files)\n",
                "num_partial = len(partial_files)\n",
                "\n",
                "print(f\"\\n{'='*50}\")\n",
                "print(f\"TRAINING PAIRS CREATED\")\n",
                "print(f\"{'='*50}\")\n",
                "print(f\"Full point clouds: {num_full}\")\n",
                "print(f\"Partial point clouds: {num_partial}\")\n",
                "print(f\"Expected: ~{num_pc * 3} each\")\n",
                "print(f\"Unique samples: {num_full // 3}\")\n",
                "\n",
                "if num_full < 300 or num_partial < 300:\n",
                "    print(f\"‚ùå ERROR: Not enough training pairs!\")\n",
                "    sys.exit(1)\n",
                "else:\n",
                "    print(f\"‚úÖ SUCCESS - Ready for training!\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Step 8: Create Data Splits"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "import numpy as np\n",
                "from pathlib import Path\n",
                "\n",
                "training_dir = Path('data/training_objaverse')\n",
                "full_dir = training_dir / 'full'\n",
                "\n",
                "# Get unique samples\n",
                "samples = set()\n",
                "for f in full_dir.glob('*_full.npy'):\n",
                "    name = f.stem.replace('_full', '')\n",
                "    base = '_'.join(name.split('_')[:-1])\n",
                "    samples.add(base)\n",
                "\n",
                "samples = sorted(list(samples))\n",
                "print(f\"Total unique samples: {len(samples)}\")\n",
                "\n",
                "# 80/10/10 split\n",
                "np.random.seed(42)\n",
                "np.random.shuffle(samples)\n",
                "\n",
                "n = len(samples)\n",
                "train = samples[:int(0.8*n)]\n",
                "val = samples[int(0.8*n):int(0.9*n)]\n",
                "test = samples[int(0.9*n):]\n",
                "\n",
                "# Save splits\n",
                "splits_dir = training_dir / 'splits'\n",
                "splits_dir.mkdir(exist_ok=True)\n",
                "\n",
                "(splits_dir / 'train.txt').write_text('\\n'.join(train))\n",
                "(splits_dir / 'val.txt').write_text('\\n'.join(val))\n",
                "(splits_dir / 'test.txt').write_text('\\n'.join(test))\n",
                "\n",
                "print(f\"\\n{'='*50}\")\n",
                "print(f\"DATA SPLITS CREATED\")\n",
                "print(f\"{'='*50}\")\n",
                "print(f\"Train: {len(train)}\")\n",
                "print(f\"Val: {len(val)}\")\n",
                "print(f\"Test: {len(test)}\")\n",
                "print(f\"‚úÖ Ready to train!\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Step 9: Train Model (150 Epochs)\n",
                "\n",
                "‚è±Ô∏è **This takes 2-4 hours** - keep tab open!\n",
                "\n",
                "For quick test: change `--epochs 150` to `--epochs 10`"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Create checkpoint and log directories\n",
                "!mkdir -p checkpoints_objaverse logs_objaverse\n",
                "\n",
                "# Check batch size based on GPU memory\n",
                "gpu_mem_gb = torch.cuda.get_device_properties(0).total_memory / 1e9\n",
                "batch_size = 32 if gpu_mem_gb > 20 else 16 if gpu_mem_gb > 12 else 8\n",
                "\n",
                "print(f\"GPU Memory: {gpu_mem_gb:.1f} GB\")\n",
                "print(f\"Using batch size: {batch_size}\")\n",
                "print(f\"\\nStarting training...\\n\")\n",
                "\n",
                "# Train with real furniture data!\n",
                "!python scripts/train.py \\\n",
                "    --data_dir data/training_objaverse \\\n",
                "    --epochs 150 \\\n",
                "    --batch_size {batch_size} \\\n",
                "    --checkpoint_dir checkpoints_objaverse \\\n",
                "    --log_dir logs_objaverse \\\n",
                "    --device cuda\n",
                "\n",
                "print(\"\\n‚úÖ Training complete!\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Step 10: Monitor Training (TensorBoard)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "%load_ext tensorboard\n",
                "%tensorboard --logdir logs_objaverse"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Step 11: Auto-Find Best Checkpoint"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "import glob\n",
                "\n",
                "checkpoint_files = glob.glob('checkpoints_objaverse/*/best.pt')\n",
                "\n",
                "if not checkpoint_files:\n",
                "    print(\"‚ùå No checkpoint found!\")\n",
                "    CP = None\n",
                "else:\n",
                "    CP = sorted(checkpoint_files)[-1]\n",
                "    print(f\"‚úÖ Found checkpoint: {CP}\")\n",
                "    !ls -lh {CP}\n",
                "    \n",
                "    # Load and check training summary\n",
                "    summary_file = str(Path(CP).parent / 'training_summary.json')\n",
                "    if os.path.exists(summary_file):\n",
                "        with open(summary_file) as f:\n",
                "            summary = json.load(f)\n",
                "        print(f\"\\nBest val loss: {summary['best_val_loss']:.6f}\")\n",
                "        print(f\"Epochs completed: {summary['epochs_completed']}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Step 12: Test Inference"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Create test input\n",
                "import numpy as np\n",
                "\n",
                "partial = np.random.randn(2048, 3).astype(np.float32)\n",
                "partial = (partial - partial.mean(axis=0)) / partial.std()\n",
                "np.save('test_input.npy', partial)\n",
                "\n",
                "print(f\"‚úÖ Test input created: {partial.shape}\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Run inference - point cloud output\n",
                "if CP:\n",
                "    !python scripts/infer.py \\\n",
                "        --checkpoint {CP} \\\n",
                "        --input test_input.npy \\\n",
                "        --output completed.npy \\\n",
                "        --class_id 0 \\\n",
                "        --device cuda\n",
                "    \n",
                "    print(\"\\n‚úÖ Inference complete!\")\n",
                "    \n",
                "    # Check output\n",
                "    completed = np.load('completed.npy')\n",
                "    print(f\"Output shape: {completed.shape}\")\n",
                "else:\n",
                "    print(\"‚ùå No checkpoint available for inference\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Step 13: Export GLB (With Real Furniture Details!)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Export to GLB mesh - should have LEGS now!\n",
                "if CP:\n",
                "    !python scripts/infer.py \\\n",
                "        --checkpoint {CP} \\\n",
                "        --input test_input.npy \\\n",
                "        --output chair_with_legs.glb \\\n",
                "        --export_mesh \\\n",
                "        --class_id 0 \\\n",
                "        --device cuda\n",
                "    \n",
                "    print(\"\\n‚úÖ GLB exported!\")\n",
                "    print(\"üì• Download and view in 3D viewer - should see LEGS!\")\n",
                "    \n",
                "    !ls -lh chair_with_legs.glb\n",
                "else:\n",
                "    print(\"‚ùå No checkpoint available\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Step 14: Visualize Comparison"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "import matplotlib.pyplot as plt\n",
                "from mpl_toolkits.mplot3d import Axes3D\n",
                "\n",
                "if os.path.exists('completed.npy') and os.path.exists('test_input.npy'):\n",
                "    partial = np.load('test_input.npy')\n",
                "    completed = np.load('completed.npy')\n",
                "    \n",
                "    fig = plt.figure(figsize=(15, 5))\n",
                "    \n",
                "    # Input\n",
                "    ax1 = fig.add_subplot(131, projection='3d')\n",
                "    ax1.scatter(partial[:, 0], partial[:, 1], partial[:, 2], c='blue', s=1)\n",
                "    ax1.set_title('Input (Partial)', fontsize=14)\n",
                "    ax1.set_box_aspect([1,1,1])\n",
                "    \n",
                "    # Output (should have details now!)\n",
                "    ax2 = fig.add_subplot(132, projection='3d')\n",
                "    ax2.scatter(completed[:, 0], completed[:, 1], completed[:, 2], c='green', s=1)\n",
                "    ax2.set_title('Output (Completed - Real Data!)', fontsize=14)\n",
                "    ax2.set_box_aspect([1,1,1])\n",
                "    \n",
                "    # Overlay\n",
                "    ax3 = fig.add_subplot(133, projection='3d')\n",
                "    ax3.scatter(partial[:, 0], partial[:, 1], partial[:, 2], c='blue', s=1, alpha=0.5, label='Input')\n",
                "    ax3.scatter(completed[:, 0], completed[:, 1], completed[:, 2], c='green', s=1, alpha=0.3, label='Output')\n",
                "    ax3.set_title('Comparison', fontsize=14)\n",
                "    ax3.legend()\n",
                "    ax3.set_box_aspect([1,1,1])\n",
                "    \n",
                "    plt.tight_layout()\n",
                "    plt.show()\n",
                "    \n",
                "    print(f\"Input points: {len(partial)}\")\n",
                "    print(f\"Output points: {len(completed)}\")\n",
                "else:\n",
                "    print(\"‚ö†Ô∏è Output files not found - run inference first\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Step 15: Download Everything"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "from google.colab import files\n",
                "\n",
                "if CP:\n",
                "    print(\"üì• Downloading files...\\n\")\n",
                "    \n",
                "    # Download checkpoint\n",
                "    print(\"1. Checkpoint (best.pt)\")\n",
                "    files.download(CP)\n",
                "    \n",
                "    # Download GLB\n",
                "    if os.path.exists('chair_with_legs.glb'):\n",
                "        print(\"2. GLB file (chair with LEGS!)\")\n",
                "        files.download('chair_with_legs.glb')\n",
                "    \n",
                "    # Download training summary\n",
                "    summary_file = str(Path(CP).parent / 'training_summary.json')\n",
                "    if os.path.exists(summary_file):\n",
                "        print(\"3. Training summary\")\n",
                "        files.download(summary_file)\n",
                "    \n",
                "    print(\"\\n‚úÖ All files downloaded!\")\n",
                "else:\n",
                "    print(\"‚ùå No checkpoint to download\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "##Quality Check"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "import json\n",
                "\n",
                "summary_files = glob.glob('checkpoints_objaverse/*/training_summary.json')\n",
                "\n",
                "if summary_files:\n",
                "    with open(summary_files[-1]) as f:\n",
                "        summary = json.load(f)\n",
                "    \n",
                "    print(\"=\"*50)\n",
                "    print(\"TRAINING RESULTS - REAL FURNITURE DATA\")\n",
                "    print(\"=\"*50)\n",
                "    print(f\"Best val loss: {summary['best_val_loss']:.6f}\")\n",
                "    print(f\"Final train loss: {summary['train_losses'][-1]:.6f}\")\n",
                "    print(f\"Epochs: {summary['epochs_completed']}\")\n",
                "    print(\"\\n\" + \"=\"*50)\n",
                "    print(\"QUALITY ASSESSMENT\")\n",
                "    print(\"=\"*50)\n",
                "    \n",
                "    val_loss = summary['best_val_loss']\n",
                "    \n",
                "    if val_loss < 0.005:\n",
                "        print(f\"‚úÖ EXCELLENT ({val_loss:.6f})\")\n",
                "        print(\"   Your model should produce high-quality furniture!\")\n",
                "        print(\"   Expected: Chairs with legs, realistic details\")\n",
                "    elif val_loss < 0.01:\n",
                "        print(f\"‚úÖ VERY GOOD ({val_loss:.6f})\")\n",
                "        print(\"   Production-ready quality\")\n",
                "    elif val_loss < 0.05:\n",
                "        print(f\"‚úÖ GOOD ({val_loss:.6f})\")\n",
                "        print(\"   Usable for beta/MVP\")\n",
                "    else:\n",
                "        print(f\"‚ö†Ô∏è  OK ({val_loss:.6f})\")\n",
                "        print(\"   May need more training or data\")\n",
                "    \n",
                "    print(\"\\nüìã Next steps:\")\n",
                "    print(\"   1. Download chair_with_legs.glb\")\n",
                "    print(\"   2. View in 3D viewer - check for LEGS!\")\n",
                "    print(\"   3. If good ‚Üí Week 2: SAM-3D benchmarking\")\n",
                "    print(\"   4. If issues ‚Üí Retrain with more data\")\n",
                "else:\n",
                "    print(\"‚ùå No training summary found\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## üéâ Training Complete!\n",
                "\n",
                "### What You Have:\n",
                "- ‚úÖ Model trained on **real furniture** (not placeholder boxes!)\n",
                "- ‚úÖ Chairs should have **LEGS** now\n",
                "- ‚úÖ Tables should have **proper structure** \n",
                "- ‚úÖ Production-ready checkpoint\n",
                "\n",
                "### Next Steps:\n",
                "1. **Download GLB** and view in [3dviewer.net](https://3dviewer.net)\n",
                "2. **Verify quality** - chairs should have legs!\n",
                "3. **Week 2** - SAM-3D benchmarking\n",
                "4. **Week 3-4** - Deploy to production!\n",
                "\n",
                "### Expected Quality:\n",
                "- **Better than placeholder** - realistic furniture shapes\n",
                "- **Production viable** - can show to customers\n",
                "- **AR-ready** - GLB files work in AR viewers\n",
                "\n",
                "üöÄ **You've built production-quality AI!**"
            ]
        }
    ],
    "metadata": {
        "accelerator": "GPU",
        "colab": {
            "provenance": [],
            "gpuType": "T4"
        },
        "kernelspec": {
            "display_name": "Python 3",
            "name": "python3"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 0
}