{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# SV-SCN Production Training\n",
                "\n",
                "**Train a production-quality 3D shape completion model**\n",
                "\n",
                "- 500 training samples\n",
                "- 150 epochs (~2-3 hours)\n",
                "- Automatic checkpoint handling\n",
                "- Quality validation\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Step 1: Check GPU"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "!nvidia-smi\n",
                "\n",
                "import torch\n",
                "print(f\"\\nPyTorch: {torch.__version__}\")\n",
                "print(f\"CUDA: {torch.cuda.is_available()}\")\n",
                "if torch.cuda.is_available():\n",
                "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
                "else:\n",
                "    print(\"‚ö†Ô∏è Enable GPU: Runtime ‚Üí Change runtime type ‚Üí GPU\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Step 2: Clone Project\n",
                "\n",
                "**IMPORTANT:** Replace `YOUR_GITHUB_USERNAME` with your username!"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Clone repository\n",
                "!git clone https://github.com/ashish-frozo/frozo-3d-model.git\n",
                "%cd frozo-3d-model\n",
                "\n",
                "# Verify structure\n",
                "!ls -la svscn/ scripts/ 2>&1 || echo \"‚ùå Project structure not found!\""
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Step 3: Install Dependencies"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "!pip install -q open3d>=0.17.0 trimesh>=4.0.0 scipy>=1.10.0\n",
                "!pip install -q objaverse>=0.1.7 tensorboard>=2.14.0\n",
                "\n",
                "print(\"‚úÖ Dependencies installed\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Step 4: Test Imports"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "from svscn.models import SVSCN\n",
                "from svscn.config import default_config\n",
                "\n",
                "print(f\"‚úÖ Imports successful\")\n",
                "print(f\"Model version: {default_config.VERSION}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Step 5: Generate 500 Training Samples\n",
                "\n",
                "This will create ~1500 training pairs (500 samples √ó 3 views)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Generate placeholder data\n",
                "!python -m svscn.data.shapenet \\\n",
                "    --placeholder \\\n",
                "    --output_dir data/shapenet_500 \\\n",
                "    --samples_per_category 167\n",
                "\n",
                "print(\"\\n‚úÖ Step 1/3: Generated meshes\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Preprocess to point clouds\n",
                "!python -m svscn.data.preprocess \\\n",
                "    --input_dir data/shapenet_500 \\\n",
                "    --output_dir data/processed_500 \\\n",
                "    --num_points 8192\n",
                "\n",
                "print(\"\\n‚úÖ Step 2/3: Converted to point clouds\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Generate training pairs\n",
                "!python -m svscn.data.augment \\\n",
                "    --input_dir data/processed_500 \\\n",
                "    --output_dir data/training_500 \\\n",
                "    --views 3\n",
                "\n",
                "print(\"\\n‚úÖ Step 3/3: Created training pairs\")\n",
                "\n",
                "# Verify\n",
                "import subprocess\n",
                "result = subprocess.run(['find', 'data/training_500', '-name', '*.npy'], \n",
                "                       capture_output=True, text=True)\n",
                "num_files = len(result.stdout.strip().split('\\n'))\n",
                "print(f\"Total training files: {num_files}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Step 6: Create Data Splits"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "import numpy as np\n",
                "from pathlib import Path\n",
                "\n",
                "training_dir = Path('data/training_500')\n",
                "full_dir = training_dir / 'full'\n",
                "\n",
                "# Get unique samples\n",
                "samples = set()\n",
                "for f in full_dir.glob('*_full.npy'):\n",
                "    name = f.stem.replace('_full', '')\n",
                "    base = '_'.join(name.split('_')[:-1])\n",
                "    samples.add(base)\n",
                "\n",
                "samples = sorted(list(samples))\n",
                "print(f\"Total unique samples: {len(samples)}\")\n",
                "\n",
                "# 80/10/10 split\n",
                "np.random.seed(42)\n",
                "np.random.shuffle(samples)\n",
                "\n",
                "n = len(samples)\n",
                "train = samples[:int(0.8*n)]\n",
                "val = samples[int(0.8*n):int(0.9*n)]\n",
                "test = samples[int(0.9*n):]\n",
                "\n",
                "# Save splits\n",
                "splits_dir = training_dir / 'splits'\n",
                "splits_dir.mkdir(exist_ok=True)\n",
                "\n",
                "(splits_dir / 'train.txt').write_text('\\n'.join(train))\n",
                "(splits_dir / 'val.txt').write_text('\\n'.join(val))\n",
                "(splits_dir / 'test.txt').write_text('\\n'.join(test))\n",
                "\n",
                "print(f\"‚úÖ Splits: {len(train)} train, {len(val)} val, {len(test)} test\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Step 7: Train Production Model (150 epochs)\n",
                "\n",
                "‚è±Ô∏è **This will take 2-3 hours** - keep tab open!\n",
                "\n",
                "To run a quick test first (10 epochs, ~10 min), change `--epochs 150` to `--epochs 10`"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Production training - 150 epochs\n",
                "!python scripts/train.py \\\n",
                "    --data_dir data/training_500 \\\n",
                "    --epochs 150 \\\n",
                "    --batch_size 32 \\\n",
                "    --checkpoint_dir checkpoints_prod \\\n",
                "    --log_dir logs_prod \\\n",
                "    --device cuda\n",
                "\n",
                "print(\"\\n‚úÖ Training complete!\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Step 8: Monitor Training (TensorBoard)\n",
                "\n",
                "Run this in parallel while training to watch progress"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "%load_ext tensorboard\n",
                "%tensorboard --logdir logs_prod"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Step 9: Find Best Checkpoint\n",
                "\n",
                "**Automatic checkpoint detection** - no hardcoding!"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "import glob\n",
                "\n",
                "# Find all checkpoint files\n",
                "checkpoint_files = glob.glob('checkpoints_prod/*/best.pt')\n",
                "\n",
                "if not checkpoint_files:\n",
                "    print(\"‚ùå No checkpoint found! Make sure training completed.\")\n",
                "    CHECKPOINT_PATH = None\n",
                "else:\n",
                "    # Use the most recent one\n",
                "    CHECKPOINT_PATH = sorted(checkpoint_files)[-1]\n",
                "    print(f\"‚úÖ Found checkpoint: {CHECKPOINT_PATH}\")\n",
                "    \n",
                "    # Get info\n",
                "    !ls -lh {CHECKPOINT_PATH}"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Step 10: Test Inference"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Create test input\n",
                "import numpy as np\n",
                "\n",
                "partial = np.random.randn(2048, 3).astype(np.float32)\n",
                "partial = (partial - partial.mean(axis=0)) / partial.std()\n",
                "np.save('test_partial.npy', partial)\n",
                "\n",
                "print(f\"‚úÖ Created test input: {partial.shape}\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Run inference with automatic checkpoint\n",
                "if CHECKPOINT_PATH:\n",
                "    !python scripts/infer.py \\\n",
                "        --checkpoint {CHECKPOINT_PATH} \\\n",
                "        --input test_partial.npy \\\n",
                "        --output completed.npy \\\n",
                "        --class_id 0 \\\n",
                "        --device cuda\n",
                "    \n",
                "    print(\"\\n‚úÖ Inference complete!\")\n",
                "else:\n",
                "    print(\"‚ùå Cannot run inference - no checkpoint found\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Step 11: Visualize Results"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "import numpy as np\n",
                "import matplotlib.pyplot as plt\n",
                "from mpl_toolkits.mplot3d import Axes3D\n",
                "\n",
                "partial = np.load('test_partial.npy')\n",
                "completed = np.load('completed.npy')\n",
                "\n",
                "fig = plt.figure(figsize=(15, 5))\n",
                "\n",
                "# Input\n",
                "ax1 = fig.add_subplot(131, projection='3d')\n",
                "ax1.scatter(partial[:, 0], partial[:, 1], partial[:, 2], c='blue', s=1)\n",
                "ax1.set_title('Input (Partial)', fontsize=14)\n",
                "ax1.set_box_aspect([1,1,1])\n",
                "\n",
                "# Output\n",
                "ax2 = fig.add_subplot(132, projection='3d')\n",
                "ax2.scatter(completed[:, 0], completed[:, 1], completed[:, 2], c='green', s=1)\n",
                "ax2.set_title('Output (Completed)', fontsize=14)\n",
                "ax2.set_box_aspect([1,1,1])\n",
                "\n",
                "# Overlay\n",
                "ax3 = fig.add_subplot(133, projection='3d')\n",
                "ax3.scatter(partial[:, 0], partial[:, 1], partial[:, 2], c='blue', s=1, alpha=0.5, label='Input')\n",
                "ax3.scatter(completed[:, 0], completed[:, 1], completed[:, 2], c='green', s=1, alpha=0.3, label='Output')\n",
                "ax3.set_title('Comparison', fontsize=14)\n",
                "ax3.legend()\n",
                "ax3.set_box_aspect([1,1,1])\n",
                "\n",
                "plt.tight_layout()\n",
                "plt.show()\n",
                "\n",
                "print(f\"Input: {len(partial)} points\")\n",
                "print(f\"Output: {len(completed)} points\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Step 12: Export as GLB (AR-ready)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Export to GLB with automatic checkpoint\n",
                "if CHECKPOINT_PATH:\n",
                "    !python scripts/infer.py \\\n",
                "        --checkpoint {CHECKPOINT_PATH} \\\n",
                "        --input test_partial.npy \\\n",
                "        --output completed_prod.glb \\\n",
                "        --export_mesh \\\n",
                "        --class_id 0 \\\n",
                "        --device cuda\n",
                "    \n",
                "    print(\"\\n‚úÖ GLB export complete!\")\n",
                "else:\n",
                "    print(\"‚ùå Cannot export - no checkpoint found\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Step 13: Download Files"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "from google.colab import files\n",
                "\n",
                "if CHECKPOINT_PATH:\n",
                "    # Download checkpoint\n",
                "    files.download(CHECKPOINT_PATH)\n",
                "    print(f\"Downloaded: {CHECKPOINT_PATH}\")\n",
                "    \n",
                "    # Download GLB\n",
                "    files.download('completed_prod.glb')\n",
                "    print(\"Downloaded: completed_prod.glb\")\n",
                "    \n",
                "    print(\"\\n‚úÖ All files downloaded!\")\n",
                "else:\n",
                "    print(\"‚ùå No files to download - training not complete\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Step 14: Quality Metrics\n",
                "\n",
                "Check if model meets production criteria"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "import json\n",
                "\n",
                "# Find training summary\n",
                "summary_files = glob.glob('checkpoints_prod/*/training_summary.json')\n",
                "\n",
                "if summary_files:\n",
                "    with open(summary_files[-1]) as f:\n",
                "        summary = json.load(f)\n",
                "    \n",
                "    print(\"=\" * 50)\n",
                "    print(\"TRAINING SUMMARY\")\n",
                "    print(\"=\" * 50)\n",
                "    print(f\"Best val loss: {summary['best_val_loss']:.4f}\")\n",
                "    print(f\"Final train loss: {summary['train_losses'][-1]:.4f}\")\n",
                "    print(f\"Epochs completed: {summary['epochs_completed']}\")\n",
                "    print(\"\\n\" + \"=\" * 50)\n",
                "    print(\"QUALITY CHECK\")\n",
                "    print(\"=\" * 50)\n",
                "    \n",
                "    # Check criteria\n",
                "    val_loss = summary['best_val_loss']\n",
                "    target_loss = 0.5\n",
                "    \n",
                "    if val_loss < target_loss:\n",
                "        print(f\"‚úÖ Val loss ({val_loss:.4f}) < {target_loss} - EXCELLENT!\")\n",
                "    elif val_loss < 0.8:\n",
                "        print(f\"‚ö†Ô∏è  Val loss ({val_loss:.4f}) - GOOD, could be better\")\n",
                "    else:\n",
                "        print(f\"‚ùå Val loss ({val_loss:.4f}) - needs more training\")\n",
                "    \n",
                "    print(\"\\nüìã Manual checks needed:\")\n",
                "    print(\"   - View GLB in 3D viewer\")\n",
                "    print(\"   - Check for holes at 1m distance\")\n",
                "    print(\"   - Verify backside geometry looks plausible\")\n",
                "    print(\"   - Test in AR on phone (iOS/Android)\")\n",
                "else:\n",
                "    print(\"‚ùå No training summary found\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Summary\n",
                "\n",
                "### ‚úÖ What You've Done:\n",
                "1. Trained production-quality model (500 samples, 150 epochs)\n",
                "2. Automatic checkpoint handling (no hardcoding!)\n",
                "3. Quality validation and metrics\n",
                "4. Downloaded model and 3D outputs\n",
                "\n",
                "### üöÄ Next Steps:\n",
                "- Test on real furniture data\n",
                "- Deploy as REST API\n",
                "- Integrate with your platform\n",
                "- Compare with SAM-3D (optional)\n",
                "\n",
                "### üíª Local Inference:\n",
                "```bash\n",
                "python scripts/infer.py \\\n",
                "    --checkpoint best.pt \\\n",
                "    --input your_data.npy \\\n",
                "    --output result.glb \\\n",
                "    --export_mesh \\\n",
                "    --device cpu\n",
                "```"
            ]
        }
    ],
    "metadata": {
        "accelerator": "GPU",
        "colab": {
            "provenance": [],
            "gpuType": "T4"
        },
        "kernelspec": {
            "display_name": "Python 3",
            "name": "python3"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 0
}